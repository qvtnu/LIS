{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import theano\n",
    "import lasagne\n",
    "import numpy as np\n",
    "from theano import tensor as T\n",
    "import time\n",
    "\n",
    "train = pd.read_hdf(\"train.h5\", \"train\")\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "labelnumber=5   # 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53461, 100)\n",
      "feature selected new dimension is 79\n",
      "train set size (40792, 79) (40792,)\n",
      "Eval set size (4532, 79) (4532,)\n",
      "Test set size (8137, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:31: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:32: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:33: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:34: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# convert from dataframe to numpy array\n",
    "y_train = train.y.as_matrix()\n",
    "\n",
    "X_train = train.iloc[:,1:101].as_matrix()\n",
    "X_test= test.as_matrix()\n",
    "X = np.concatenate((X_train, X_test), 0)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# drop feature with too small standard deviation  (<0.005))\n",
    "drop_feature = np.where(X.std(axis=0)<0.005)\n",
    "X = np.delete(X, drop_feature, axis=1)\n",
    "\n",
    "dimension = X.shape[1]\n",
    "print (\"feature selected new dimension is\", int(dimension))\n",
    "\n",
    "#scale the X\n",
    "from sklearn.preprocessing import scale\n",
    "X = scale(X, axis=0)\n",
    "X_train = X [0:X_train.shape[0],:]\n",
    "X_test  = X [X_train.shape[0]:X.shape[0],:]\n",
    "\n",
    "# divide into train set and eval set\n",
    "num_sample = y_train.shape[0]\n",
    "\n",
    "# take 1/10 for evaluation\n",
    "num_eval = np.rint(num_sample/10)  \n",
    "num_train = num_sample-num_eval\n",
    "\n",
    "\n",
    "X_val = X_train[num_train:num_sample]\n",
    "X_train = X_train[0:num_train]\n",
    "y_val = y_train[num_train:num_sample].astype(int) \n",
    "y_train = y_train[0:num_train].astype(int) \n",
    "\n",
    "print('train set size', X_train.shape, y_train.shape)\n",
    "print('Eval set size', X_val.shape, y_val.shape)\n",
    "print('Test set size', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network 1: normal mlp(multilayer perceptron)\n",
    "def build_mlp(input_var=None):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, dimension),\n",
    "                                     input_var=input_var)\n",
    "    \n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "    \n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "        l_in_drop, num_units=400,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "        l_hid1_drop, num_units=500,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "    \n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_hid2_drop, num_units=5,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network 2:\n",
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, dimension),\n",
    "                                    input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 5, nonlinearity=softmax)\n",
    "    return network    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network 3: convolutional neural network\n",
    "def build_cnn(input_var=None):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, dimension),\n",
    "                                        input_var=input_var)\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "        network, num_filters=32, filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "        network, num_filters=32, filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(network, p=.5),\n",
    "        num_units=256,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(network, p=.5),\n",
    "        num_units=5,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.dmatrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model\n",
    "\n",
    "#network = build_mlp(input_var)\n",
    "\n",
    "network = build_custom_mlp(input_var, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5)\n",
    "\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "\n",
    "# define loss function\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "# update weights\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "\n",
    "\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "predict_fn = theano.function([input_var], T.argmax(prediction, axis=1, keepdims=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 9.049s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.647460\n",
      "  validation accuracy:\t\t81.27 %\n",
      "Epoch 2 of 100 took 6.323s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.620924\n",
      "  validation accuracy:\t\t82.53 %\n",
      "Epoch 3 of 100 took 7.147s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.613876\n",
      "  validation accuracy:\t\t82.84 %\n",
      "Epoch 4 of 100 took 6.291s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.601132\n",
      "  validation accuracy:\t\t83.69 %\n",
      "Epoch 5 of 100 took 7.125s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.598075\n",
      "  validation accuracy:\t\t83.49 %\n",
      "Epoch 6 of 100 took 7.361s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.593177\n",
      "  validation accuracy:\t\t84.16 %\n",
      "Epoch 7 of 100 took 9.794s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.580798\n",
      "  validation accuracy:\t\t84.80 %\n",
      "Epoch 8 of 100 took 8.440s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.579916\n",
      "  validation accuracy:\t\t84.42 %\n",
      "Epoch 9 of 100 took 8.384s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.578803\n",
      "  validation accuracy:\t\t84.31 %\n",
      "Epoch 10 of 100 took 8.471s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.573063\n",
      "  validation accuracy:\t\t84.71 %\n",
      "Epoch 11 of 100 took 8.119s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.573259\n",
      "  validation accuracy:\t\t84.60 %\n",
      "Epoch 12 of 100 took 8.274s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.571146\n",
      "  validation accuracy:\t\t84.58 %\n",
      "Epoch 13 of 100 took 8.671s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.568725\n",
      "  validation accuracy:\t\t84.67 %\n",
      "Epoch 14 of 100 took 8.903s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.569941\n",
      "  validation accuracy:\t\t84.76 %\n",
      "Epoch 15 of 100 took 8.506s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.571590\n",
      "  validation accuracy:\t\t84.89 %\n",
      "Epoch 16 of 100 took 8.096s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.565663\n",
      "  validation accuracy:\t\t84.80 %\n",
      "Epoch 17 of 100 took 8.580s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.559976\n",
      "  validation accuracy:\t\t85.56 %\n",
      "Epoch 18 of 100 took 10.453s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.558050\n",
      "  validation accuracy:\t\t85.22 %\n",
      "Epoch 19 of 100 took 11.070s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.563132\n",
      "  validation accuracy:\t\t85.13 %\n",
      "Epoch 20 of 100 took 9.656s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.556602\n",
      "  validation accuracy:\t\t85.24 %\n",
      "Epoch 21 of 100 took 9.178s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.559776\n",
      "  validation accuracy:\t\t84.93 %\n",
      "Epoch 22 of 100 took 8.352s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.556741\n",
      "  validation accuracy:\t\t85.51 %\n",
      "Epoch 23 of 100 took 10.732s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.554039\n",
      "  validation accuracy:\t\t85.22 %\n",
      "Epoch 24 of 100 took 9.508s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.554835\n",
      "  validation accuracy:\t\t85.33 %\n",
      "Epoch 25 of 100 took 8.571s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.553077\n",
      "  validation accuracy:\t\t85.27 %\n",
      "Epoch 26 of 100 took 7.797s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.550943\n",
      "  validation accuracy:\t\t85.56 %\n",
      "Epoch 27 of 100 took 8.257s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.547929\n",
      "  validation accuracy:\t\t85.38 %\n",
      "Epoch 28 of 100 took 8.045s\n",
      "  training loss:\t\t0.000000\n",
      "  validation loss:\t\t0.547009\n",
      "  validation accuracy:\t\t86.13 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4734cdea8e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batchsize = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = np.int64(0)\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        tmp = train_fn(inputs, targets)\n",
    "        train_err += tmp.astype(np.int32) \n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        tmp = val_fn(inputs.astype(np.int32), targets.astype(np.int32))\n",
    "        err, acc = tmp\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val_fn(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Prediction tset size', (8137L,))\n"
     ]
    }
   ],
   "source": [
    "y_test = predict_fn(X_test)\n",
    "print('Prediction tset size', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"sample.csv\")\n",
    "sub['y'] = y_test\n",
    "sub.head()\n",
    "sub.to_csv('mlp_800_800_200epoch.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
